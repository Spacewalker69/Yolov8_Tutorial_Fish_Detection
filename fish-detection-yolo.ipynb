{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/umairalam567/fish-detection-yolo?scriptVersionId=144429380\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-27T07:39:13.908059Z","iopub.execute_input":"2023-09-27T07:39:13.908786Z","iopub.status.idle":"2023-09-27T07:39:14.284839Z","shell.execute_reply.started":"2023-09-27T07:39:13.908745Z","shell.execute_reply":"2023-09-27T07:39:14.28374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating and Copying Directories in Kaggle Environment\n\nThis code is designed to manage directories and copy data within the Kaggle environment.\n\n### Libraries Imported:\n\n- **os**: Provides a way of using operating system dependent functionality, such as reading or writing to the file system.\n- **shutil**: Offers a higher-level file operation interface which includes functions to copy and remove directories.\n\n### Directory Management:\n\n- The code first checks if the directory `/kaggle/working/fish-tracking-dataset` exists.\n- If the directory doesn't exist, it creates the directory using the `os.makedirs()` function. *(Note: This part of the code is commented out.)*\n\n### Data Copying:\n\n- The `shutil.copytree()` function is used to copy the entire directory tree from `/kaggle/input/fish-tracking-dataset` to `/kaggle/working/fish-tracking-dataset`. This means all files and sub-directories within the source directory will be copied to the destination directory.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport os\n#if not os.path.exists('/kaggle/working/fish-tracking-dataset'):\n #         os.makedirs('/kaggle/working/fish-tracking-dataset')\nshutil.copytree('/kaggle/input/fish-tracking-dataset', '/kaggle/working/fish-tracking-dataset')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:39:41.246596Z","iopub.execute_input":"2023-09-27T07:39:41.24698Z","iopub.status.idle":"2023-09-27T07:39:45.953814Z","shell.execute_reply.started":"2023-09-27T07:39:41.246951Z","shell.execute_reply":"2023-09-27T07:39:45.952741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting XML Annotations to YOLO Format\n\nThis code snippet is designed to convert bounding box annotations from XML format (commonly used in datasets like Pascal VOC) to the YOLO (You Only Look Once) format.\n\n### Libraries Imported:\n\n- **os**: Provides functionalities to interact with the operating system, like creating directories.\n- **xml.etree.ElementTree**: Used for parsing and creating XML data.\n\n### Function: `xml_to_yolo`\n\n- **Parameters**:\n  - `xml_file`: Path to the XML file containing annotations.\n  - `class_id`: ID corresponding to the object class (e.g., fish).\n\n- **Functionality**:\n  - Parses the XML file to extract image details and bounding box annotations.\n  - For each bounding box, it calculates the center coordinates, width, and height normalized with respect to the image dimensions.\n  - Constructs a line in the YOLO format for each bounding box and saves these lines to a `.txt` file named after the image.\n\n### Directory Setup:\n\n- Checks if the directory `/kaggle/working/train_txt/labels` exists. If not, it creates the directory.\n- The YOLO formatted annotations are saved in this directory with filenames corresponding to the image names.\n\n### Execution:\n\n- The directory containing XML annotations is set to `/kaggle/working/fish-tracking-dataset`.\n- The class ID for the \"fish\" label is defined as 0.\n- The code then iterates through each XML file in the directory and converts its annotations to the YOLO format using the `xml_to_yolo` function.\n\n","metadata":{}},{"cell_type":"code","source":"from IPython.lib.display import exists\nimport os\nimport xml.etree.ElementTree as ET\n\ndef xml_to_yolo(xml_file, class_id):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n\n    for image_elem in root.findall('image'):\n        image_id = image_elem.get('id')\n        image_name = image_elem.get('name')\n        image_name = image_name.split('images/')[1]\n        image_name = image_name.split('.')[0]\n        print(image_name)\n        image_width = int(image_elem.get('width'))\n        image_height = int(image_elem.get('height'))\n\n        yolo_lines = []\n        for box_elem in image_elem.findall('box'):\n            label = box_elem.get('label')\n            occluded = int(box_elem.get('occluded'))\n            xtl = float(box_elem.get('xtl'))\n            ytl = float(box_elem.get('ytl'))\n            xbr = float(box_elem.get('xbr'))\n            ybr = float(box_elem.get('ybr'))\n\n            # Calculate center coordinates, width, and height\n            center_x = (xtl + xbr) / (2 * image_width)\n            center_y = (ytl + ybr) / (2 * image_height)\n            width = (xbr - xtl) / image_width\n            height = (ybr - ytl) / image_height\n\n            # Create a YOLO line\n            yolo_line = f\"{class_id} {center_x} {center_y} {width} {height}\"\n\n            # Append the YOLO line to the list\n            yolo_lines.append(yolo_line)\n\n        # Save YOLO annotations to a text file with the same name as the image_id\n        if not os.path.exists('/kaggle/working/train_txt/labels'):\n          os.makedirs('/kaggle/working/train_txt/labels')\n        txt_file_path = os.path.join('/kaggle/working/train_txt/labels', f\"{image_name}.txt\")\n        with open(txt_file_path, 'w') as txt_file:\n            txt_file.write('\\n'.join(yolo_lines))\n\n# Directory containing XML annotations\nxml_dir = '/kaggle/working/fish-tracking-dataset'\n\n# Define the class ID for the \"fish\" label\nclass_id = 0\n\n# Iterate through XML files and convert to YOLO format\nfor xml_filename in os.listdir(xml_dir):\n    if xml_filename.endswith('.xml'):\n        xml_path = os.path.join(xml_dir, xml_filename)\n        xml_to_yolo(xml_path, class_id)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:39:45.955844Z","iopub.execute_input":"2023-09-27T07:39:45.956216Z","iopub.status.idle":"2023-09-27T07:39:45.994805Z","shell.execute_reply.started":"2023-09-27T07:39:45.956182Z","shell.execute_reply":"2023-09-27T07:39:45.993828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Data into Training and Validation Sets\n\nThis code snippet is designed to split the dataset into training and validation sets based on a specified ratio. The data consists of images and their corresponding YOLO annotation files.\n\n### Libraries Imported:\n\n- **os**: Provides functionalities to interact with the operating system, like creating directories.\n- **shutil**: Offers a higher-level file operation interface which includes functions to copy and move files.\n\n### Function: `split_train_val_data`\n\n- **Parameters**:\n  - `txt_files_dir`: Directory containing YOLO annotation files.\n  - `train_dir`: Directory to store training data.\n  - `val_dir`: Directory to store validation data.\n  - `val_split_ratio`: Fraction of data to be used for validation (default is 0.2 or 20%).\n\n- **Functionality**:\n  - Creates directories for storing training and validation images and labels.\n  - Shuffles the list of annotation files to ensure randomness.\n  - Calculates the number of validation samples based on the `val_split_ratio`.\n  - Moves the validation annotation files to the validation directory and copies the corresponding images.\n  - Copies the training annotation files to the training directory and moves the corresponding images.\n\n### Execution:\n\n- The directories for YOLO annotation files, training data, and validation data are specified.\n- The `split_train_val_data` function is then called to split the data into training and validation sets based on a 20% validation split ratio.\n\n","metadata":{}},{"cell_type":"code","source":"import random\n\n\ndef split_train_val_data(txt_files_dir, train_dir, val_dir, val_split_ratio=0.2):\n    # Create train and validation directories if they don't exist\n    os.makedirs(os.path.join(train_dir, 'images'), exist_ok=True)\n    os.makedirs(os.path.join(train_dir, 'labels'), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, 'images'), exist_ok=True)\n    os.makedirs(os.path.join(val_dir, 'labels'), exist_ok=True)\n\n    txt_files = os.listdir(txt_files_dir)\n    random.shuffle(txt_files)\n\n    num_val_samples = int(val_split_ratio * len(txt_files))\n    val_files = txt_files[:num_val_samples]\n    train_files = txt_files[num_val_samples:]\n\n    for val_file in val_files:\n        src_txt_path = os.path.join(txt_files_dir, val_file)\n        dest_txt_path = os.path.join(val_dir, 'labels', val_file)\n        shutil.move(src_txt_path, dest_txt_path)\n\n        # Assuming your image filenames match the annotation filenames with the .jpg extension\n        image_filename = os.path.splitext(val_file)[0] + '.jpg'\n        src_img_path = os.path.join('/kaggle/working/fish-tracking-dataset/images/', image_filename)\n        dest_img_path = os.path.join(val_dir, 'images', image_filename)\n        shutil.copy(src_img_path, dest_img_path)\n\n    for train_file in train_files:\n        src_txt_path = os.path.join(txt_files_dir, train_file)\n        dest_txt_path = os.path.join(train_dir, 'labels', train_file)\n        shutil.copy(src_txt_path, dest_txt_path)\n\n        # Assuming your image filenames match the annotation filenames with the .jpg extension\n        image_filename = os.path.splitext(train_file)[0] + '.jpg'\n        src_img_path = os.path.join('/kaggle/working/fish-tracking-dataset/images/', image_filename)\n        dest_img_path = os.path.join(train_dir, 'images', image_filename)\n        shutil.move(src_img_path, dest_img_path)\n\n# Directory containing YOLO annotation files\ntxt_files_dir = '/kaggle/working/train_txt/labels'\ntrain_dir = '/kaggle/working/train'\nval_dir = '/kaggle/working/val'\n\n# Split the data into train and validation sets\nsplit_train_val_data(txt_files_dir, train_dir, val_dir, val_split_ratio=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:40:14.096134Z","iopub.execute_input":"2023-09-27T07:40:14.096478Z","iopub.status.idle":"2023-09-27T07:40:14.132862Z","shell.execute_reply.started":"2023-09-27T07:40:14.096449Z","shell.execute_reply":"2023-09-27T07:40:14.131961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:40:31.171739Z","iopub.execute_input":"2023-09-27T07:40:31.17209Z","iopub.status.idle":"2023-09-27T07:40:46.691342Z","shell.execute_reply.started":"2023-09-27T07:40:31.172063Z","shell.execute_reply":"2023-09-27T07:40:46.690176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyyaml\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:44:29.03868Z","iopub.execute_input":"2023-09-27T07:44:29.039195Z","iopub.status.idle":"2023-09-27T07:44:41.043744Z","shell.execute_reply.started":"2023-09-27T07:44:29.039162Z","shell.execute_reply":"2023-09-27T07:44:41.042455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading YOLO Models with Ultralytics\n\nThis code snippet demonstrates how to load YOLO (You Only Look Once) models using the Ultralytics library.\n\n### Libraries Imported:\n\n- **ultralytics**: The Ultralytics library provides functionalities for the YOLO object detection framework.\n\n### Loading Models:\n\nThere are multiple ways to load a YOLO model using Ultralytics:\n\n1. **Building a New Model from YAML**:\n   - `model = YOLO('yolov8n.yaml')`: This line of code builds a new YOLO model based on the architecture specified in the 'yolov8n.yaml' file. *(Note: This line is commented out in the provided code.)*\n\n2. **Loading a Pretrained Model**:\n   - `model = YOLO('yolov8s.pt')`: This line loads a pretrained YOLO model from the 'yolov8s.pt' file. Using pretrained models is recommended when training on a new dataset as it can leverage the knowledge from the pretrained weights.\n\n3. **Building from YAML and Transferring Weights**:\n   - `model = YOLO('yolov8n.yaml').load('yolov8n.pt')`: This line first builds a YOLO model from the 'yolov8n.yaml' file and then transfers the weights from the 'yolov8n.pt' file. *(Note: This line is commented out in the provided code.)*\n\n","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load a model\n#model = YOLO('yolov8n.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8s.pt')  # load a pretrained model (recommended for training)\n#model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:40:46.693732Z","iopub.execute_input":"2023-09-27T07:40:46.694094Z","iopub.status.idle":"2023-09-27T07:40:53.757973Z","shell.execute_reply.started":"2023-09-27T07:40:46.694054Z","shell.execute_reply":"2023-09-27T07:40:53.756992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating and Saving a YAML Configuration File\n\nThis code snippet demonstrates how to define a configuration in Python, and then save it as a YAML (YAML Ain't Markup Language) file using the `yaml` library.\n\n### Libraries Imported:\n\n- **yaml**: The `yaml` library provides functionalities for YAML parsing, which is a human-readable data serialization format.\n\n### Configuration Data:\n\n- A Python dictionary named `config` is defined, which contains various configuration settings:\n  - `path`: Working directory path.\n  - `train`: Relative path to the training images.\n  - `val`: Relative path to the validation images.\n  - `nc`: Number of classes.\n  - `names`: List of class names.\n  - `fl_gamma`: (Commented out) Can be used to specify a focal loss gamma value if needed.\n\n### Saving to YAML File:\n\n- The desired file path for the YAML file is specified as `yaml_file_path`.\n- The `with` statement is used to open this file in write mode.\n- The `yaml.dump()` function is then used to write the `config` dictionary to the file in YAML format.\n- Finally, a confirmation message is printed to indicate the successful saving of the configuration to the YAML file.\n\n","metadata":{}},{"cell_type":"code","source":"import yaml\n\n# Define your configuration data as a Python dictionary\nconfig = {\n    'path': '/kaggle/working',\n    'train': 'train/images',  # train images (relative to 'path') 4 images\n    'val': 'val/images',      # val images (relative to 'path') 4 images\n    'nc': 1,                   # Number of classes\n    'names': ['Fish'],         # Class names\n    # 'fl_gamma': 2.0          # Uncomment this line if needed\n}\n\n# Define the file path where you want to save the YAML file\nyaml_file_path = '/kaggle/working/config.yaml'  # Update with your desired path and file name\n\n# Write the YAML data to the file\nwith open(yaml_file_path, 'w') as yaml_file:\n    yaml.dump(config, yaml_file)\n\nprint(f\"YAML configuration file saved at: {yaml_file_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:45:44.946993Z","iopub.execute_input":"2023-09-27T07:45:44.947376Z","iopub.status.idle":"2023-09-27T07:45:44.956926Z","shell.execute_reply.started":"2023-09-27T07:45:44.947344Z","shell.execute_reply":"2023-09-27T07:45:44.955881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the YOLO Model with Ultralytics\n\nThis code snippet demonstrates how to train a YOLO model using the Ultralytics library based on a specified configuration.\n\n### Model Training:\n\nThe `train()` method of the YOLO model is called with the following parameters:\n\n- **data**: Path to the YAML configuration file that contains dataset paths, class names, and other related information.\n- **epochs**: Number of training epochs. An epoch is one complete forward and backward pass of all the training examples.\n- **imgsz**: Image size for training. All training images will be resized to this size.\n- **pretrained**: If set to `True`, the model will use pretrained weights. This can help in achieving better accuracy faster.\n- **name**: Name of the training run. Useful for distinguishing between different training sessions.\n- **patience**: Number of epochs with no improvement after which training will be stopped. Helps in preventing overfitting.\n- **flipud**: Probability of flipping an image vertically during data augmentation.\n- **batch**: Batch size for training. Determines the number of samples that will be used in each iteration to update the model's weights.\n- **optimizer**: Optimization algorithm to be used. In this case, Stochastic Gradient Descent (SGD) is used.\n- **augment**: If set to `True`, data augmentation techniques will be applied to the training images. This can help in improving the model's generalization.\n\nThe result of the training process is stored in the `results` variable.\n\n","metadata":{}},{"cell_type":"code","source":"results = model.train(data='/kaggle/working/config.yaml',\n                      epochs=220,\n                      imgsz=640,\n                      pretrained = True ,\n                      name= \"fish_small\",\n                      patience = 35,\n                      flipud=0.5,\n                      batch = 32,\n                      optimizer = 'SGD',\n                      augment = True\n                      )","metadata":{"execution":{"iopub.status.busy":"2023-09-27T07:45:47.391305Z","iopub.execute_input":"2023-09-27T07:45:47.391659Z","iopub.status.idle":"2023-09-27T08:04:19.363385Z","shell.execute_reply.started":"2023-09-27T07:45:47.391629Z","shell.execute_reply":"2023-09-27T08:04:19.361843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validating the YOLO Model with Ultralytics\n\nThis code snippet demonstrates how to validate a trained YOLO model using the Ultralytics library.\n\n### Model Loading:\n\n- **model**: The YOLO model is loaded using the `YOLO` class from Ultralytics. The path to the best weights from a previous training run is provided to initialize the model.\n\n### Model Validation:\n\n- **results**: The `val()` method of the YOLO model is called without any parameters to validate the model using the default settings.\n\n### Validation with Different Confidence Thresholds:\n\nThe model is further validated using different confidence thresholds to understand its performance at various levels of confidence:\n\n- The `for` loop iterates over a list of confidence thresholds: `0.25`, `0.15`, and `0.05`.\n- For each confidence threshold, the `val()` method is called with the following parameters:\n  - **name**: A custom name for the validation run, indicating the confidence threshold used.\n  - **conf**: The confidence threshold. Only detections with a confidence score above this threshold will be considered.\n  - **iou**: Intersection over Union (IoU) threshold set to `0.8`. It determines how much overlap an accurate detection should have with the ground truth for it to be considered correct.\n\nThis validation process helps in understanding the model's performance at different confidence levels and can guide decisions on the optimal confidence threshold to use for detections.\n\n","metadata":{}},{"cell_type":"code","source":"model = YOLO('/kaggle/working/runs/detect/fish_small2/weights/best.pt')\nresults = model.val()\nfor i in [0.25,0.15,0.05]:\n  results = model.val(name= f'cofidence: {i}', conf= i , iou=0.8)","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:05:40.178352Z","iopub.execute_input":"2023-09-27T08:05:40.178772Z","iopub.status.idle":"2023-09-27T08:06:09.503448Z","shell.execute_reply.started":"2023-09-27T08:05:40.178739Z","shell.execute_reply":"2023-09-27T08:06:09.501527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = YOLO('/kaggle/working/runs/detect/fish_small2/weights/best.pt')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:06:32.179471Z","iopub.execute_input":"2023-09-27T08:06:32.179872Z","iopub.status.idle":"2023-09-27T08:06:32.30831Z","shell.execute_reply.started":"2023-09-27T08:06:32.17984Z","shell.execute_reply":"2023-09-27T08:06:32.306129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Essential Libraries for Image Processing and Visualization\n\nThis code snippet imports various Python libraries that are commonly used for image processing, data manipulation, and visualization.\n\n### Libraries Imported:\n\n- **pandas**: A powerful library for data manipulation and analysis, particularly with structured data.\n- **numpy**: Fundamental package for numerical computations in Python.\n- **PIL**: Python Imaging Library (also known as Pillow) is used for opening, manipulating, and saving image files.\n- **Image**: A module from PIL to specifically handle image operations.\n- **IPython.display**: Provides functionalities to display objects within Jupyter.\n- **matplotlib.pyplot**: Provides a MATLAB-like plotting framework in Python.\n- **glob**: Useful for retrieving files/pathnames matching a specified pattern.\n- **random**: Provides functions to generate random numbers.\n- **cv2**: OpenCV library, a powerful tool for computer vision tasks.\n- **warnings**: Used to control the behavior of warning messages in Python.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport PIL \nfrom PIL import Image\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport random\nimport cv2\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:06:44.374538Z","iopub.execute_input":"2023-09-27T08:06:44.375501Z","iopub.status.idle":"2023-09-27T08:06:44.388581Z","shell.execute_reply.started":"2023-09-27T08:06:44.375466Z","shell.execute_reply":"2023-09-27T08:06:44.385466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Displaying Random Sample Images from Validation Set\n\nThis code snippet is designed to randomly select and display images from the validation set.\n\n### Setting Up Paths and Parameters:\n\n- **root_path**: Specifies the path where validation images are located. The `*` at the end of the path is a wildcard that matches all files in the directory.\n- **num_samples**: Number of random sample images to display, set to 4.\n\n### Retrieving and Sampling Images:\n\n- **images_data**: Uses `glob(root_path)` to retrieve all file paths that match the specified pattern, effectively getting paths to all images in the directory.\n- **random_image**: Randomly selects `num_samples` image paths from `images_data` using `random.sample()`.\n\n### Displaying Images:\n\n- A matplotlib figure is created with a specified size.\n- A `for` loop iterates over the range of `num_samples`:\n  - For each iteration, a subplot is created.\n  - The image is read using `cv2.imread()` and displayed using `plt.imshow()`.\n  - The axis labels are turned off for a cleaner display.\n\n### Visual Output:\n\n- The output will be a 2x2 grid displaying four randomly selected images from the validation set.\n\n","metadata":{}},{"cell_type":"code","source":"root_path = '/kaggle/working/val/images/*'\nnum_samples = 4\nimages_data = glob(root_path)\nrandom_image = random.sample(images_data, num_samples)\n\nplt.figure(figsize=(12,10))\nfor i in range(num_samples):\n    plt.subplot(2,2,i+1)\n    plt.imshow(cv2.imread(random_image[i]))\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:07:02.039696Z","iopub.execute_input":"2023-09-27T08:07:02.040095Z","iopub.status.idle":"2023-09-27T08:07:03.814438Z","shell.execute_reply.started":"2023-09-27T08:07:02.040063Z","shell.execute_reply":"2023-09-27T08:07:03.813264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting and Displaying Object Detections on Sample Images\n\nThis code snippet is designed to predict object detections on a set of randomly selected images using the trained YOLO model and then display the results.\n\n### Predicting Detections:\n\n- An empty list `images` is initialized to store the processed images with bounding boxes.\n- A `for` loop iterates over the range of `num_samples`:\n  - For each image, the YOLO model's `predict()` method is called to get the detection results.\n  - The results include bounding boxes (`box`), class names (`names`), and other attributes.\n  - A series of print statements display information about each detected object:\n    - Total number of detected fish in the image.\n    - Label of the detected object (e.g., \"Fish\").\n    - Coordinates of the bounding box.\n    - Confidence score of the detection.\n\n### Storing Processed Images:\n\n- The `output.plot()` method is used to generate an image with bounding boxes drawn around detected objects.\n- The color channels of the image are reversed (`[:, :, ::-1]`) to convert from BGR to RGB format (as OpenCV reads images in BGR format by default).\n- The processed image is then appended to the `images` list.\n\n### Visual Output:\n\n- The `images` list will contain the randomly selected sample images with bounding boxes drawn around detected objects, ready for visualization.\n\n","metadata":{}},{"cell_type":"code","source":"images = []\nfor i in range(num_samples):\n    yolo_outputs = model.predict(random_image[i])\n    output = yolo_outputs[0]\n    box = output.boxes\n    names = output.names\n    print('**********************')\n    for j in range(len(box)):\n        labels = names[box.cls[j].item()]\n        coordinates = box.xyxy[j].tolist()\n        confidence = np.round(box.conf[j].item(), 2)\n        print(f'In this image {len(box)} fish has been detected.')\n        print(f'Fish {j + 1} is: {labels}')\n        print(f'Coordinates are: {coordinates}')\n        print(f'Confidence is: {confidence}')\n        print('-------')\n        \n    # Store the image in the 'images' list\n    images.append(output.plot()[:, :, ::-1])","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:07:18.895144Z","iopub.execute_input":"2023-09-27T08:07:18.895505Z","iopub.status.idle":"2023-09-27T08:07:19.54809Z","shell.execute_reply.started":"2023-09-27T08:07:18.895475Z","shell.execute_reply":"2023-09-27T08:07:19.546835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nfor i, img in enumerate(images):\n    plt.subplot(2, 2, i + 1)\n    plt.imshow(img)\n    plt.axis('off') ","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:07:33.023132Z","iopub.execute_input":"2023-09-27T08:07:33.023561Z","iopub.status.idle":"2023-09-27T08:07:34.772449Z","shell.execute_reply.started":"2023-09-27T08:07:33.023529Z","shell.execute_reply":"2023-09-27T08:07:34.771266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Training and Validation Losses\n\nThis code snippet is designed to visualize the training and validation losses over epochs for both box and class losses.\n\n### Data Preprocessing:\n\n- **Removing Spaces**: The leading and trailing spaces from the column names of the `result` dataframe are removed using the `str.strip()` method.\n\n### Extracting Relevant Data:\n\n- **epoch_column**: Extracts the epoch numbers.\n- **box_train_losses** and **box_val_losses**: Extracts the training and validation box losses, respectively.\n- **cls_train_losses** and **cls_val_losses**: Extracts the training and validation class losses, respectively.\n\n### Plotting Losses:\n\n- A figure with two subplots is created using `plt.figure()` and `plt.subplot()`.\n- The `ggplot` style is applied to the plots for better visualization.\n  \n1. **Box Losses**:\n   - The first subplot displays the training and validation box losses over epochs.\n   - The `plt.plot()` function is used to plot the box losses against the epochs.\n   - A grid is added for better readability.\n   - Labels, title, and legend are added for clarity.\n\n2. **Class Losses**:\n   - The second subplot displays the training and validation class losses over epochs.\n   - The `plt.plot()` function is used to plot the class losses against the epochs.\n   - A grid is added for better readability.\n   - Labels, title, and legend are added for clarity.\n\nFinally, `plt.show()` is called to display the plots.\n\n### Visual Output:\n\n- The output will be two line plots side by side:\n  - The left plot shows the training and validation box losses over epochs.\n  - The right plot shows the training and validation class losses over epochs.\n\n","metadata":{}},{"cell_type":"code","source":"result = pd.read_csv('/kaggle/working/runs/detect/fish_small2/results.csv')\nresult.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:08:29.731408Z","iopub.execute_input":"2023-09-27T08:08:29.731793Z","iopub.status.idle":"2023-09-27T08:08:29.775861Z","shell.execute_reply.started":"2023-09-27T08:08:29.731762Z","shell.execute_reply":"2023-09-27T08:08:29.774421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove leading and trailing spaces from column names\nresult.columns = result.columns.str.strip()\n\nepoch_column = result['epoch']\nbox_train_losses = result['train/box_loss']\nbox_val_losses = result['val/box_loss']\ncls_train_losses = result['train/cls_loss']\ncls_val_losses = result['val/cls_loss']\n\nplt.figure(figsize=(12,5))\nplt.style.use('ggplot')  # You can choose a style you prefer\nplt.subplot(1,2,1)\nplt.plot(epoch_column, box_train_losses, label='train_losses')\nplt.plot(epoch_column, box_val_losses, label='val_losses')\nplt.grid(True, linestyle='--', linewidth=0.5, color='gray')# Add a grid\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train and Validation Box Losses')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(epoch_column, cls_train_losses, label='train_losses')\nplt.plot(epoch_column, cls_val_losses, label='val_losses')\nplt.grid(True, linestyle='--', linewidth=0.5, color='gray')# Add a grid\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train and Validation Class Losses')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:08:38.783918Z","iopub.execute_input":"2023-09-27T08:08:38.784403Z","iopub.status.idle":"2023-09-27T08:08:39.452485Z","shell.execute_reply.started":"2023-09-27T08:08:38.784369Z","shell.execute_reply":"2023-09-27T08:08:39.450142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.imshow(cv2.imread('/kaggle/working/runs/detect/fish_small2/results.png'))\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-09-27T08:09:10.767452Z","iopub.execute_input":"2023-09-27T08:09:10.767838Z","iopub.status.idle":"2023-09-27T08:09:11.654894Z","shell.execute_reply.started":"2023-09-27T08:09:10.767806Z","shell.execute_reply":"2023-09-27T08:09:11.653971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}